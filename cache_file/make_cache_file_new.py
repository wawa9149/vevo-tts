#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
< ì°¸ê³ ì‚¬í•­ / ìŠ¤í¬ë¦½íŠ¸ ê°œìš” >

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Amphionì˜ EmiliaDataset êµ¬í˜„ì´ ìš”êµ¬í•˜ëŠ” 4ê°œ ìºì‹œ(pkl)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- wav_paths_cache.pkl       : MNT_PATH ê¸°ì¤€ ìƒëŒ€ê²½ë¡œë¡œ ëœ wav ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
- json_paths_cache.pkl      : (ë¬¸ìžì—´ìƒ *_fixzh.json) ê²½ë¡œ ë¦¬ìŠ¤íŠ¸  â€» ì‹¤ì œë¡œëŠ” {prefix}.json íŒŒì¼ì„ ì—½ë‹ˆë‹¤
- duration_cache.pkl        : ê° ìƒ˜í”Œ ì˜¤ë””ì˜¤ ê¸¸ì´(ì´ˆ)
- phone_count_cache.pkl     : ê° ìƒ˜í”Œ í…ìŠ¤íŠ¸ì˜ í•œê¸€ ìŒì ˆ ìˆ˜(ê³µë°±/êµ¬ë‘ì  ì œê±°)
ì£¼ì˜: ì‹¤ì œë¡œëŠ” g2pë¡œ ë³€í™˜ í›„ phoneme ê°œìˆ˜ë¥¼ ì„¸ì•¼í•˜ì§€ë§Œ í˜„ìž¬ ì½”ë“œëŠ” ë‹¨ìˆœížˆ text ì˜ ê°œìˆ˜ë¥¼ ì„¸ë„ë¡ ë˜ì–´ ìžˆìŠµë‹ˆë‹¤. ì‚¬ìš©í•˜ì‹œëŠ” g2pì— ë”°ë¼ ìˆ˜ì • ë¶€íƒë“œë¦½ë‹ˆë‹¤.

[ ì „ì œ í´ë” êµ¬ì¡° (MNT_PATH = /hdd_ext/hdd2/sujin/MAGO/mago-dataset) ]
/hdd_ext/hdd2/sujin/MAGO/mago-dataset/
â”œâ”€ aihub_ko/
â”‚  â”œâ”€ dataset_large/
â”‚  â”‚  â””â”€ {SPEAKER}/
â”‚  â”‚     â”œâ”€ wav_48000/
â”‚  â”‚     â”‚  â””â”€ {SPEAKER}_{INDEX}.wav      (ì˜ˆ: F0003_101665.wav)
â”‚  â”‚     â””â”€ script.txt                    (ì›ë¬¸/ì–µì–‘ ë§ˆí¬ í¬í•¨ í…ìŠ¤íŠ¸)
â”‚  â”œâ”€ dataset_small/
â”‚  â”‚  â””â”€ {SPEAKER}/
â”‚  â”‚     â””â”€ wav_48000/ ë˜ëŠ” wav48000/
â”‚  â”‚        â””â”€ {SPEAKER}_{INDEX}.wav      (ì˜ˆ: F2001_000001.wav, í´ë”ëª… í‘œê¸°ëŠ” í˜¼ìš© ê°€ëŠ¥)
â”‚  â””â”€ dataset_small_transcripts/          (Whisper ì „ì‚¬ JSON ëª¨ìŒ; íŒŒì¼ë³„ ìŠ¤í‚¤ë§ˆì— rel_path, text í¬í•¨)
â””â”€ emilia_ko/
   â””â”€ ko/
      â”œâ”€ *.wav                             (ì˜ˆ: KO_B00002_S04731_W000001.wav)
      â””â”€ *.json (ìžˆì„ ìˆ˜ë„ ìžˆìŒ)          (ë™ì¼ stemì˜ ë³´ì¡° í…ìŠ¤íŠ¸)

â€» emilia_ko/ko_formatted/ëŠ” ë³¸ ìŠ¤í¬ë¦½íŠ¸ê°€ ìžë™ ìƒì„±í•˜ëŠ” "ê·œê²©í™” ìŠ¤í…Œì´ì§• í´ë”"ìž…ë‹ˆë‹¤.
   - ì›ë³¸ ko/ ì˜ íŒŒì¼ëª…ì€ ë í† í°ì´ 'W000001'ì²˜ëŸ¼ ì •ìˆ˜ê°€ ì•„ë‹ˆë¼ EmiliaDatasetê³¼ ì•ˆ ë§žìŠµë‹ˆë‹¤.
   - ë³¸ ìŠ¤í¬ë¦½íŠ¸ëŠ” ko/ ë¥¼ ì½ì–´ ko_formatted/ ì— {audio_name}_{ì •ìˆ˜}.wav í˜•íƒœë¡œ ë§í¬/ë³µì‚¬í•´ ë‘¡ë‹ˆë‹¤.
   - ê°™ì€ í´ë”ì— {audio_name}.json (indexâ†’meta)ë„ ìƒì„±í•©ë‹ˆë‹¤.

[ ë°ì´í„° ì†ŒìŠ¤ë³„ ì²˜ë¦¬ ìš”ì•½ ]
1) dataset_large
   - ê° ìŠ¤í”¼ì»¤ì˜ script.txtë¥¼ íŒŒì‹±í•´ì„œ ë°œí™” ID â†’ í…ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.
   - í…ìŠ¤íŠ¸ ì¤„ì—ì„œ '||', 'M', 'LH', 'HL', 'LL', 'HH' ë“±ì˜ ì–µì–‘/êµ¬ë¶„ ë§ˆí¬ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
   - wav_48000 í´ë” ì•ˆì— {SPEAKER}.json(ì‹¤ì œ ì½ížˆëŠ” íŒŒì¼) ì €ìž¥: {index: {language, text, start, end, phone_count}}
   - phone_count = í…ìŠ¤íŠ¸ ë‚´ "í•œê¸€ ìŒì ˆ ìˆ˜"(ì •ê·œì‹ [ê°€-íž£]ë§Œ ì¹´ìš´íŠ¸)

2) dataset_small
   - aihub_ko/dataset_small_transcripts/ ì˜ Whisper JSONë“¤ì„ ì½ì–´ rel_path â†’ wavë¥¼ ì°¾ê³  textë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
   - durationì€ JSONì— ì—†ìœ¼ë©´ segmentsì˜ end ìµœëŒ“ê°’, ê·¸ëž˜ë„ ì—†ìœ¼ë©´ librosaë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
   - ìŠ¤í”¼ì»¤ë³„ wav í´ë”(wav_48000 ë˜ëŠ” wav48000)ì— {SPEAKER}.json ì €ìž¥(ìœ„ì™€ ë™ì¼ ìŠ¤í‚¤ë§ˆ).
   - phone_count = í•œê¸€ ìŒì ˆ ìˆ˜.

3) emilia_ko
   - emilia_ko/ko/ ì˜ *.wav íŒŒì¼ëª…ì„ íŒŒì‹±í•˜ì—¬ ë í† í° 'W000001' â†’ 1ì²˜ëŸ¼ ì •ìˆ˜ indexë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
   - emilia_ko/ko_formatted/ ì— {audio_name}_{index}.wav ì‹¬ë³¼ë¦­ ë§í¬(ë¶ˆê°€ ì‹œ ë³µì‚¬) ìƒì„±.
   - ê°™ì€ ì´ë¦„ì˜ ì›ë³¸ jsonì´ ko/ì— ìžˆìœ¼ë©´ textë¥¼ ì‚¬ìš©, ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìžì—´.
   - ko_formatted/ ì— {audio_name}.json ì €ìž¥(ìœ„ì™€ ë™ì¼ ìŠ¤í‚¤ë§ˆ).
   - phone_count = í•œê¸€ ìŒì ˆ ìˆ˜.

[ ìºì‹œ íŒŒì¼ê³¼ EmiliaDataset ì—°ë™ ]
- wav_paths_cache.pkl / json_paths_cache.pkl ì—ëŠ” "MNT_PATH ê¸°ì¤€ ìƒëŒ€ê²½ë¡œ(ì•žì— / í¬í•¨)"ê°€ ì €ìž¥ë©ë‹ˆë‹¤.
  EmiliaDatasetëŠ” ì‹¤ì œ ë¡œë”© ì‹œ self.mnt_path + rel_path ë¡œ í•©ì¹©ë‹ˆë‹¤.
- json_paths_cache.pklì—ëŠ” *_fixzh.json ë¬¸ìžì—´ì„ ë„£ì§€ë§Œ, EmiliaDatasetê°€ ë‚´ë¶€ì—ì„œ "_fixzh"ë¥¼ ì œê±°í•˜ì—¬
  ê°™ì€ í´ë”ì˜ {prefix}.json íŒŒì¼ì„ ì—½ë‹ˆë‹¤.  â†’ ì‹¤ì œ {prefix}.json íŒŒì¼ì´ ì¡´ìž¬í•´ì•¼ í•©ë‹ˆë‹¤.
- ë„¤ ê°œ ë¦¬ìŠ¤íŠ¸ëŠ” ê°™ì€ ì¸ë±ìŠ¤ê°€ í•œ ìƒ˜í”Œì„ êµ¬ì„±í•©ë‹ˆë‹¤.

[ í™˜ê²½ ì„¤ì • / ì‹¤í–‰ ]
- ì½”ë“œ ìƒë‹¨ì˜ MNT_PATH, CACHE_PATH ê°’ì„ í™˜ê²½ì— ë§žê²Œ ì„¤ì •í•˜ì„¸ìš”.
- librosa, tqdm í•„ìš”.
- emilia ìª½ì—ì„œ ë§í¬ê°€ ë¶ˆê°€í•œ íŒŒì¼ì‹œìŠ¤í…œì´ë©´ build_emilia(copy_instead_of_symlink=True)ë¡œ ë³µì‚¬ ì‚¬ìš© ê°€ëŠ¥.
- í˜„ìž¬ íŒŒì¼ì—” ê°œë°œ íŽ¸ì˜ë¥¼ ìœ„í•œ pdb.set_trace()ê°€ ë“¤ì–´ê°€ ìžˆìœ¼ë‹ˆ, ë°°í¬/ì‹¤í–‰ ì‹œ ì œê±°í•˜ì„¸ìš”.

[ ì£¼ì˜ ì‚¬í•­ ]
- dataset_smallì˜ ì‹¤ì œ í´ë”ëª…ì´ wav_48000ê³¼ wav48000ì´ í˜¼ìš©ë˜ì–´ë„ ìŠ¤í¬ë¦½íŠ¸ê°€ ë³´ì •í•©ë‹ˆë‹¤.
- dataset_largeì˜ ë°œí™” ID ë§¤ì¹­ì€ stem ê·¸ëŒ€ë¡œ ë˜ëŠ” ë ì¸ë±ìŠ¤ë¥¼ 6ìžë¦¬ 0íŒ¨ë”©í•œ í‚¤ë„ í•¨ê»˜ ì‹œë„í•©ë‹ˆë‹¤.
- duration ê¸°ë³¸ ìƒ˜í”Œë§ì€ íŒŒì¼ SR ê·¸ëŒ€ë¡œ(librosa.load sr=None) ì‚¬ìš©.
- phone_countëŠ” ê³µë°±/êµ¬ë‘ì /ì˜ë¬¸/ìˆ«ìžë¥¼ ì œì™¸í•œ í•œê¸€ ìŒì ˆë§Œ ì¹´ìš´íŠ¸í•©ë‹ˆë‹¤.

ì´ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ í›„ CACHE_PATH í´ë”ì— 4ê°œ pklì´ ìƒì„±ë˜ë©°,
EmiliaDataset(MNT_PATH/CACHE_PATH ë™ì¼ ì„¤ì •)ì—ì„œ cache_type="path"ë¡œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
"""


import os, re, json, pickle, shutil
from pathlib import Path
from collections import defaultdict
from tqdm import tqdm
import librosa

import re

def count_hangul_syllables(text: str) -> int:
    # í•œê¸€ ìŒì ˆ ë¸”ë¡ë§Œ ì¹´ìš´íŠ¸ (ê³µë°±/êµ¬ë‘ì /ì˜ë¬¸ ì œì™¸)
    return len(re.findall(r"[ê°€-íž£]", text or ""))


import pdb; pdb.set_trace()
# ===================== ê²½ë¡œ ì„¤ì • =====================
MNT_PATH   = "/hdd_ext/hdd2/sujin/MAGO/mago-dataset"            # EmiliaDatasetì˜ MNT_PATHì™€ ë™ì¼
CACHE_PATH = "/hdd_ext/hdd2/sujin/MAGO/mago-dataset_cache"  # pkl 4ê°œ ì €ìž¥ ìœ„ì¹˜

AIHUB_ROOT          = f"{MNT_PATH}/aihub_ko"
DATASET_LARGE       = f"{AIHUB_ROOT}/dataset_large"
DATASET_SMALL       = f"{AIHUB_ROOT}/dataset_small"
SMALL_JSON_ROOT     = f"{AIHUB_ROOT}/dataset_small_transcripts"  # ðŸ‘ˆ Whisper ì „ì‚¬ JSON í´ë”
EMILIA_KO_ROOT      = f"{MNT_PATH}/emilia_ko/ko"
EMILIA_FMT_ROOT     = f"{MNT_PATH}/emilia_ko/ko_formatted"

SR_FOR_DURATION = None  # Noneì´ë©´ ì›ë³¸ SR ì‚¬ìš©

# ===================== ìœ í‹¸ =====================
def ensure_dir(p): Path(p).mkdir(parents=True, exist_ok=True)

def rel_to_mnt(p: str) -> str:
    return p[len(MNT_PATH):] if p.startswith(MNT_PATH) else p

def load_duration(wav_path: str):
    try:
        y, sr = librosa.load(wav_path, sr=SR_FOR_DURATION)
        return float(len(y)/sr)
    except Exception as e:
        print(f"[WARN] librosa fail: {wav_path} ({e})"); return None

def save_json(obj, path):
    ensure_dir(os.path.dirname(path))
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False)

def looks_int(tok: str) -> bool:
    try: int(tok); return True
    except: return False

def extract_index_from_stem(stem: str):
    toks = stem.split("_")
    return int(toks[-1]) if looks_int(toks[-1]) else None

def strip_prosody_marks(line: str) -> str:
    s = re.sub(r"\|+", " ", line)
    s = re.sub(r"\b(M|LH|HL|LL|HH)\b", "", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

# ===================== 1) dataset_large =====================
def parse_script_txt(script_path: Path):
    id2text = {}
    if not script_path.exists(): return id2text
    lines = script_path.read_text(encoding="utf-8", errors="ignore").splitlines()
    i = 0
    while i < len(lines):
        header = lines[i].strip()
        m = re.match(r"^([A-Za-z0-9_]+)\s+", header)
        if m:
            utt_id = m.group(1)
            j = i + 1
            first_content = None
            while j < len(lines):
                L = lines[j].strip()
                if not L: break
                if first_content is None:
                    first_content = L
                j += 1
            if first_content:
                text = strip_prosody_marks(first_content)
                id2text[utt_id] = text
            i = j + 1
        else:
            i += 1
    return id2text

def build_large():
    wav_rel, json_rel, durs, phones = [], [], [], []
    for spk_dir in tqdm(sorted(Path(DATASET_LARGE).glob("*")), desc="dataset_large"):
        if not spk_dir.is_dir(): continue
        wavdir = spk_dir / "wav_48000"
        script = spk_dir / "script.txt"
        if not wavdir.is_dir(): continue
        speaker = spk_dir.name
        id2text = parse_script_txt(script)
        json_path_real = wavdir / f"{speaker}.json"

        meta = {}
        for wav in sorted(wavdir.glob("*.wav")):
            stem = wav.stem
            idx = extract_index_from_stem(stem)
            if idx is None: continue
            dur = load_duration(str(wav))
            if dur is None: continue

            # stem, ë˜ëŠ” 0íŒ¨ë”© 6ìžë¦¬ í‚¤ë¡œ ë§¤ì¹­
            text = id2text.get(stem, "")
            if not text:
                toks = stem.split("_")
                head = "_".join(toks[:-1])
                text = id2text.get(f"{head}_{int(toks[-1]):06d}", "")

            meta[idx] = {
                "language": "ko",
                "text": text,
                "start": 0.0, "end": dur,
                "phone_count": count_hangul_syllables(text)
            }


        if meta:
            save_json(meta, str(json_path_real))
            for idx in sorted(meta.keys()):
                wav_file = wavdir / f"{speaker}_{idx}.wav"
                if not wav_file.exists(): continue
                wav_rel.append(rel_to_mnt(str(wav_file)))
                json_rel.append(rel_to_mnt(str(json_path_real.with_name(f"{speaker}_fixzh.json"))))
                durs.append(meta[idx]["end"])
                phones.append(meta[idx]["phone_count"])
    return wav_rel, json_rel, durs, phones

# ===================== 2) dataset_small (Whisper ì „ì‚¬ ì‚¬ìš©) =====================
def iter_small_transcripts():
    """dataset_small_transcripts í´ë”ì˜ ëª¨ë“  jsonì„ yield."""
    root = Path(SMALL_JSON_ROOT)
    if not root.exists(): return
    for jf in root.rglob("*.json"):
        try:
            data = json.load(open(jf, "r", encoding="utf-8"))
            yield data
        except Exception as e:
            print(f"[WARN] read fail: {jf} ({e})")

def build_small_from_transcripts():
    """
    rel_pathë¡œ wav ì°¾ê¸°:
      wav_abs = DATASET_SMALL / rel_path
    duration:
      - json["duration"]ê°€ ìžˆìœ¼ë©´ ì‚¬ìš©
      - ì—†ìœ¼ë©´ segmentsì˜ end ìµœëŒ“ê°’
      - ê·¸ëž˜ë„ ì•ˆë˜ë©´ librosaë¡œ ê³„ì‚°
    """
    wav_rel, json_rel, durs, phones = [], [], [], []
    # ìŠ¤í”¼ì»¤ë³„ë¡œ ëª¨ì•„ í•œ í´ë”(wav_48000)ì— {SPEAKER}.json ìƒì„±
    speaker_to_meta = defaultdict(dict)

    for rec in tqdm(list(iter_small_transcripts()), desc="small_transcripts"):
        rel_path = rec.get("rel_path") or ""
        text = rec.get("text") or ""
        segments = rec.get("segments") or []
        duration = rec.get("duration")
        if duration is None and segments:
            try:
                duration = max(float(s.get("end", 0.0)) for s in segments)
            except Exception:
                duration = None

        wav_abs = Path(DATASET_SMALL) / rel_path  # ex) .../dataset_small/F2001/wav_48000/F2001_000001.wav
        if not wav_abs.exists():
            # í˜¹ì‹œ í´ë”ëª…ì´ wav48000 / wav_48000 í˜¼ìš©ì´ë©´ ë³´ì •
            wav_abs2 = Path(str(wav_abs).replace("wav_48000", "wav48000"))
            if wav_abs2.exists(): wav_abs = wav_abs2
            else: 
                print(f"[MISS] {wav_abs}"); 
                continue

        if duration is None:
            duration = load_duration(str(wav_abs))
            if duration is None: 
                continue

        stem = wav_abs.stem            # F2001_000001
        idx = extract_index_from_stem(stem)
        if idx is None: 
            continue

        speaker = wav_abs.parent.parent.name  # F2001


        speaker_to_meta[speaker][idx] = {
            "language": "ko",
            "text": text,
            "start": 0.0, "end": float(duration),
            "phone_count": count_hangul_syllables(text)
        }


    # ìŠ¤í”¼ì»¤ë³„ json ì €ìž¥ & ìºì‹œ ìˆ˜ì§‘
    for speaker, meta in speaker_to_meta.items():
        # ì‹¤ì œ wav í´ë” ê²½ë¡œ ì¶”ì • (ë‘˜ ë‹¤ ì‹œë„)
        wavdir = Path(DATASET_SMALL) / speaker / "wav_48000"
        if not wavdir.is_dir():
            wavdir = Path(DATASET_SMALL) / speaker / "wav48000"
        if not wavdir.is_dir(): 
            continue

        json_path_real = wavdir / f"{speaker}.json"
        save_json({int(k):v for k,v in meta.items()}, str(json_path_real))

        for idx in sorted(meta.keys()):
            wav_file = wavdir / f"{speaker}_{idx}.wav"
            if not wav_file.exists():
                # ê²½ìš°ì— ë”°ë¼ ì‹¤ì œ íŒŒì¼ëª…ì´ ë‹¤ë¥´ë©´ skip
                continue
            wav_rel.append(rel_to_mnt(str(wav_file)))
            json_rel.append(rel_to_mnt(str(json_path_real.with_name(f"{speaker}_fixzh.json"))))
            durs.append(meta[idx]["end"])
            phones.append(meta[idx]["phone_count"])

    return wav_rel, json_rel, durs, phones

# ===================== 3) emilia_ko ê·œê²©í™” =====================
def number_from_W(token: str):
    m = re.match(r"^[Ww]0*([0-9]+)$", token)
    return int(m.group(1)) if m else None

def build_emilia(copy_instead_of_symlink=False):
    ensure_dir(EMILIA_FMT_ROOT)
    wav_rel, json_rel, durs, phones = [], [], [], []

    for wav in tqdm(sorted(Path(EMILIA_KO_ROOT).glob("*.wav")), desc="emilia_ko"):
        stem = wav.stem  # KO_B00002_S04731_W000001
        toks = stem.split("_")
        if len(toks) < 2: continue
        idx = number_from_W(toks[-1])
        if idx is None: continue
        audio_name = "_".join(toks[:-1])

        out_dir = Path(EMILIA_FMT_ROOT)
        ensure_dir(out_dir)
        new_wav = out_dir / f"{audio_name}_{idx}.wav"

        if not new_wav.exists():
            if copy_instead_of_symlink:
                shutil.copy2(str(wav), str(new_wav))
            else:
                try: os.symlink(os.path.abspath(str(wav)), str(new_wav))
                except Exception: shutil.copy2(str(wav), str(new_wav))

        # í…ìŠ¤íŠ¸ ì†ŒìŠ¤: ê°™ì€ ì´ë¦„ì˜ json(ìžˆìœ¼ë©´ ì‚¬ìš©)
        text = ""
        src_json = wav.with_suffix(".json")
        if src_json.exists():
            try:
                jd = json.load(open(src_json, "r", encoding="utf-8"))
                if isinstance(jd, dict) and "text" in jd: text = jd["text"]
                elif isinstance(jd, dict) and "0" in jd and isinstance(jd["0"], dict) and "text" in jd["0"]:
                    text = jd["0"]["text"]
            except Exception: pass

        dur = load_duration(str(wav))
        if dur is None: continue

        json_path_real = out_dir / f"{audio_name}.json"
        try:
            meta = json.load(open(json_path_real, "r", encoding="utf-8"))
            meta = {int(k): v for k, v in meta.items()}
        except Exception:
            meta = {}

        meta[idx] = {
            "language": "ko",
            "text": text,
            "start": 0.0, "end": float(dur),
            "phone_count": count_hangul_syllables(text)
        }
        


        save_json(meta, str(json_path_real))

        wav_rel.append(rel_to_mnt(str(new_wav)))
        json_rel.append(rel_to_mnt(str(json_path_real.with_name(f"{audio_name}_fixzh.json"))))
        durs.append(float(dur))
        phones.append(count_hangul_syllables(text))

    return wav_rel, json_rel, durs, phones

# ===================== ë©”ì¸ =====================
def main():
    ensure_dir(CACHE_PATH)
    all_wavs, all_jsons, all_durs, all_phones = [], [], [], []

    # 1) dataset_large (script.txt íŒŒì‹±)
    w, j, d, p = build_large()
    all_wavs += w; all_jsons += j; all_durs += d; all_phones += p

    # 2) dataset_small (Whisper ì „ì‚¬ ì‚¬ìš©)
    w, j, d, p = build_small_from_transcripts()
    all_wavs += w; all_jsons += j; all_durs += d; all_phones += p

    # 3) emilia_ko (íŒŒì¼ëª… ê·œê²©í™” + json)
    w, j, d, p = build_emilia(copy_instead_of_symlink=False)
    all_wavs += w; all_jsons += j; all_durs += d; all_phones += p

    # 4ê°œ pkl ì €ìž¥ (EmiliaDatasetê°€ ê¸°ëŒ€í•˜ëŠ” ì´ë¦„)
    with open(os.path.join(CACHE_PATH, "wav_paths_cache.pkl"), "wb") as f:
        pickle.dump(all_wavs, f)
    with open(os.path.join(CACHE_PATH, "json_paths_cache.pkl"), "wb") as f:
        pickle.dump(all_jsons, f)
    with open(os.path.join(CACHE_PATH, "duration_cache.pkl"), "wb") as f:
        pickle.dump(all_durs, f)
    with open(os.path.join(CACHE_PATH, "phone_count_cache.pkl"), "wb") as f:
        pickle.dump(all_phones, f)

    print("âœ… Done. Cache files created at:", CACHE_PATH)
    print(f"#wavs={len(all_wavs)}  #jsons(list)={len(all_jsons)}")

if __name__ == "__main__":
    main()
